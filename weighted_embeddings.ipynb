{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 20:49:04.215201: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-21 20:49:04.215241: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "100%|██████████| 9137/9137 [00:02<00:00, 3315.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from src.preprocessing import Preprocessor\n",
    "\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "\n",
    "data = pd.read_csv(\n",
    "        \"data/data_formatted/mst_all_exploded.csv\",\n",
    "        # nrows=100,\n",
    "    )\n",
    "\n",
    "preprocessor = Preprocessor(\n",
    "    steps=[\n",
    "            \"remove_identity_child\",\n",
    "            \"remove_identity_therapist\",\n",
    "            \"remove_narration\",\n",
    "            \"lowercase\",\n",
    "            \"normalize_i\",\n",
    "            # \"remove_number\",\n",
    "            # \"remove_punctuation\",\n",
    "            \"tokenize\",\n",
    "            \"number_filter\",\n",
    "            \"punctuation_filter\",\n",
    "            \"detokenize\"\n",
    "        ],\n",
    "    n_jobs=1 # > 1 slower since operations are cheap \n",
    "    )\n",
    "\n",
    "data[\"child_sent\"] = preprocessor(data[\"child_sent\"].tolist())\n",
    "\n",
    "dataset = defaultdict(dict)\n",
    "for filename, group in data.groupby(\"file\"):\n",
    "    dataset[filename][\"sentences\"] = group[\"child_sent\"].tolist()\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: emrecan/bert-base-turkish-cased-mean-nli-stsb-tr\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  8.07it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  6.52it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 11.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 12.83it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.00it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 18.35it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 16.06it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  7.07it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.98it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  6.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.35it/s]\n",
      "Batches: 100%|██████████| 8/8 [00:00<00:00, 11.03it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.30it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  7.27it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.85it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.09it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 10.13it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.43it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.38it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.92it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.06it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 19.88it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  6.94it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 20.59it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 11.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.36it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 20.20it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 26.96it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 16.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 15.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 13.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.95it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 14.85it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.04it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.01it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.17it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.39it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 13.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.17it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  8.27it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  7.63it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.62it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 13.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.27it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.26it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 11.12it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 21.82it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 15.07it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  9.90it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  9.18it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  6.82it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  6.93it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 17.22it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  8.70it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 13.94it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 12.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 12.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.76it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 13.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.10it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.50it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  6.08it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.88it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.93it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.27it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.49it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  8.23it/s]\n",
      "Batches: 100%|██████████| 10/10 [00:01<00:00,  9.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 14.06it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.46it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 16.75it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  7.86it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 10.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  9.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 14.33it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00, 11.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.47it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  7.14it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 18.34it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 10.46it/s]\n",
      "Batches: 100%|██████████| 9/9 [00:00<00:00, 15.60it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:00<00:00,  8.23it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 11.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  6.19it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.05it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 18.52it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 10.07it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  9.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 11.06it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.83it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 12.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  7.04it/s]\n",
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 14.86it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.06it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 13.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize embedding model\n",
    "model_checkpoint = \"emrecan/bert-base-turkish-cased-mean-nli-stsb-tr\"\n",
    "embedder = SentenceTransformer(model_checkpoint)\n",
    "\n",
    "for filename in dataset.keys():\n",
    "    dataset[filename][\"vectors\"] = embedder.encode(dataset[filename][\"sentences\"]).tolist()\n",
    "\n",
    "# Save vectors and preprocessed data\n",
    "with open('data/preprocessed_vectorized_data.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weighted document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/preprocessed_vectorized_data.json\", \"r\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rule based entity detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import generate_keyword_pattern\n",
    "\n",
    "with open(\"data/keywords.json\", \"r\") as f:\n",
    "  keywords = json.load(f)\n",
    "  del keywords['cas'] # causality removed for now\n",
    "\n",
    "merged_keyword_patterns, pattern2label = generate_keyword_pattern(keywords)\n",
    "entity_detector_kwargs = {\"merged_keyword_patterns\": merged_keyword_patterns, \"pattern2label\": pattern2label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get weighted document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:weighted_bert.models:Entity detector function you have provided is being used, not initializing HuggingFace model.\n",
      "INFO:weighted_bert.models:Entity detector function you have provided is being used, not initializing HuggingFace model.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from src.utils import detect_keywords\n",
    "from weighted_bert.models import WeightedAverage, WeightedRemoval\n",
    "\n",
    "logging.basicConfig(level=\"DEBUG\")\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "w_average = WeightedAverage(entity_detector=detect_keywords, entity_detector_kwargs=entity_detector_kwargs)\n",
    "w_removal = WeightedRemoval(entity_detector=detect_keywords, entity_detector_kwargs=entity_detector_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:weighted_bert.models:================ Detecting entities ================\n",
      "INFO:weighted_bert.models:================ Calculating initial document embeddings ================\n",
      "INFO:weighted_bert.models:================ Correcting document embeddings ================\n",
      "INFO:weighted_bert.models:\tCalculating first singular vector...\n",
      "INFO:weighted_bert.models:\tCalculating corrected embeddings...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Embeddings from Weighted Average method\n",
    "for child in dataset.keys():\n",
    "    dataset[child][\"document_vector_avg\"] = w_average.get_document_embedding(\n",
    "            document=dataset[child][\"sentences\"],\n",
    "            sentence_embeddings=np.array(dataset[child]['vectors'])\n",
    "        ).tolist()\n",
    "\n",
    "# Embeddings from Weighted Removal method\n",
    "documents = [dataset[child][\"sentences\"] for child in dataset.keys()]\n",
    "collection_sentence_embeddings = [np.array(dataset[child][\"vectors\"]) for child in dataset.keys()]\n",
    "weighted_rm_embeddings = w_removal.get_document_embeddings(\n",
    "        documents=documents,\n",
    "        collection_sentence_embeddings=collection_sentence_embeddings\n",
    "    )\n",
    "\n",
    "for child, embedding in zip(dataset.keys(), weighted_rm_embeddings):\n",
    "    dataset[child][\"document_vector_rm\"] = embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "with open('data/preprocessed_vectorized_data.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e7942183f8f618af43e31b21708df3e02b5a876a688e9c8b130eac224f4e00f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
